{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (750, 2)\n",
      "Shape y_train: (750, 1)\n",
      "Shape X_test: (250, 2)\n",
      "Shape y_test: (250, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(123)\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): \n",
    "    plt.clf()\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\") \n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\") \n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_binary_clf_proba(y_actual, y_prob):\n",
    "    plt.clf()\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_actual, y_prob)\n",
    "    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "    fpr, tpr, _ = roc_curve(y_actual, y_prob)\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    auc = roc_auc_score(y_actual, y_prob)\n",
    "    print('auc: ', auc)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_binary_clf_bin(y_actual, y_bin):\n",
    "    plt.clf()\n",
    "    print(confusion_matrix(y_actual, y_bin))\n",
    "    print('precision: ', precision_score(y_actual, y_bin))\n",
    "    print('recall: ', recall_score(y_actual, y_bin))\n",
    "    print('f1-score: ', f1_score(y_actual, y_bin))\n",
    "\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=.1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.xlim([-1.5, 1.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()\n",
    "# Reshape targets to get column vector with shape (n_samples, 1)\n",
    "y = y[:, np.newaxis]\n",
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(f'Shape X_train: {X_train.shape}')\n",
    "print(f'Shape y_train: {y_train.shape}')\n",
    "print(f'Shape X_test: {X_test.shape}')\n",
    "print(f'Shape y_test: {y_test.shape}')\n",
    "\n",
    "X_train_s = X_train[:4]\n",
    "y_train_s = y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 2), (750, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DL implementations: Transpose X, Y and use ReLU as hidden layer activition function\n",
    "class NeuralNetH1_DL():\n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden1):\n",
    "        # Initialize weight matrices and bias vectors\n",
    "        self.W1 = np.random.randn(n_hidden1, n_inputs)\n",
    "        self.b1 = np.zeros((n_hidden1, 1))\n",
    "        self.W2 = np.random.randn(n_outputs, n_hidden1)\n",
    "        self.b2 = np.zeros((n_outputs, 1))\n",
    "        \n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    def relu(self, a):\n",
    "        return np.maximum(0, a)\n",
    "    \n",
    "    def relu_d(self, a):\n",
    "        x = a\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        H1_i = np.dot(self.W1, X) + self.b1\n",
    "        H1_o = self.relu(H1_i)\n",
    "        Y_i = np.dot(self.W2, H1_o) + self.b2\n",
    "        Y_o = self.sigmoid(Y_i)\n",
    "        outputs = {\n",
    "                \"H1_o\": H1_o,\n",
    "                \"Y_o\": Y_o,\n",
    "                }\n",
    "        return outputs\n",
    "    \n",
    "    def cost(self, y_true, y_predict, n_samples):\n",
    "        cost = (- 1 / n_samples) * np.sum(y_true * np.log(y_predict) + (1 - y_true) * (np.log(1 - y_predict)))\n",
    "        cost = np.squeeze(cost)\n",
    "        assert isinstance(cost, float)\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    def backward_pass(self, X, Y, n_samples, outputs):\n",
    "        Y_o = outputs['Y_o']\n",
    "        H1_o = outputs['H1_o']\n",
    "        dz2 = Y_o - Y\n",
    "        dW2 = (1 / n_samples) * np.dot(dz2, H1_o.T)\n",
    "        db2 = (1 / n_samples) * np.sum(dz2, axis = 1, keepdims = True)  \n",
    "        dz1 = np.dot(self.W2.T, dz2) * self.relu_d(H1_o)\n",
    "        dW1 = (1 / n_samples) * np.dot(dz1, X.T)\n",
    "        db1 = (1 / n_samples) * np.sum(dz1, axis = 1, keepdims = True)    \n",
    "        gradients = {\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2,\n",
    "                \"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                }\n",
    "        return gradients\n",
    "    \n",
    "    def update_weights(self, gradients, eta):\n",
    "        self.W2 = self.W2 - eta * gradients[\"dW2\"]\n",
    "        self.b2 = self.b2 - eta * gradients[\"db2\"]\n",
    "        self.W1 = self.W1 - eta * gradients[\"dW1\"]\n",
    "        self.b1 = self.b1 - eta * gradients[\"db1\"]\n",
    "\n",
    "    def train(self, X, y, n_iters=500, eta=0.3):\n",
    "#         n_samples, _ = X.shape\n",
    "        _, n_samples = X.shape\n",
    "        for i in range(n_iters):\n",
    "            outputs = self.forward_pass(X)\n",
    "            cost = self.cost(y, outputs['Y_o'], n_samples=n_samples)\n",
    "            gradients = self.backward_pass(X, y, n_samples, outputs)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "#             if i % 1 == 0:\n",
    "                print(f'Cost at iteration {i}: {np.round(cost, 4)}')\n",
    "            self.update_weights(gradients, eta)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward_pass(X)\n",
    "        y_pred = [1 if elem >= 0.5 else 0 for elem in outputs[\"Y_o\"]]\n",
    "        return np.array(y_pred)[:, np.newaxis]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        outputs = self.forward_pass(X)\n",
    "        return outputs['Y_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0: 0.7335\n",
      "Cost at iteration 100: 0.1852\n",
      "Cost at iteration 200: 0.0803\n",
      "Cost at iteration 300: 0.0522\n",
      "Cost at iteration 400: 0.0396\n",
      "Cost at iteration 500: 0.0325\n",
      "Cost at iteration 600: 0.0278\n",
      "Cost at iteration 700: 0.0245\n",
      "Cost at iteration 800: 0.022\n",
      "Cost at iteration 900: 0.0201\n",
      "Cost at iteration 1000: 0.0185\n",
      "Cost at iteration 1100: 0.0173\n",
      "Cost at iteration 1200: 0.0162\n",
      "Cost at iteration 1300: 0.0152\n",
      "Cost at iteration 1400: 0.0144\n",
      "Cost at iteration 1500: 0.0137\n",
      "Cost at iteration 1600: 0.013\n",
      "Cost at iteration 1700: 0.0125\n",
      "Cost at iteration 1800: 0.0119\n",
      "Cost at iteration 1900: 0.0115\n"
     ]
    }
   ],
   "source": [
    "n_iters = 1000\n",
    "learning_rate = 0.1\n",
    "nn = NeuralNetH1_DL(n_inputs=2, n_outputs=1, n_hidden1=6)\n",
    "# nn = NeuralNetH2(n_inputs=2, n_outputs=1, n_hidden1=6, n_hidden2=6)\n",
    "nn.train(X_train.T, y_train.T, n_iters=2000, eta=0.7)\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# plt.plot(np.arange(n_iters), costs)\n",
    "# plt.title(\"Development of cost during training\")\n",
    "# plt.xlabel(\"Number of iterations\")\n",
    "# plt.ylabel(\"Cost\")\n",
    "# plt.show()\n",
    "\n",
    "n_samples, _ = X_train.shape\n",
    "n_samples_test, _ = X_test.shape\n",
    "\n",
    "# # y_p_train = nn.predict(X_train)\n",
    "# y_p_test = nn.predict(X_test.T)\n",
    "\n",
    "# # evaluate_binary_clf_bin(y_train, y_p_train)\n",
    "# evaluate_binary_clf_bin(y_test, y_p_test)\n",
    "\n",
    "# # y_proba_train = nn.predict_proba(X_train)\n",
    "# y_proba_test = nn.predict_proba(X_test.T)\n",
    "\n",
    "# # evaluate_binary_clf_proba(y_train, y_proba_train)\n",
    "# evaluate_binary_clf_proba(y_test, y_proba_test)\n",
    "# n_test_samples, _ = X_test.shape\n",
    "# y_predict = nn.predict(X_test.T)\n",
    "# print(f\"Classification accuracy on test set: {(np.sum(y_predict == y_test)/n_test_samples)*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetH1():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden1):\n",
    "        # Initialize weight matrices and bias vectors\n",
    "        self.W0 = np.random.randn(n_inputs, n_hidden1)\n",
    "        self.b0 = np.zeros((1, n_hidden1))\n",
    "        self.W1 = np.random.randn(n_hidden1, n_outputs)\n",
    "        self.b1 = np.zeros((1, n_outputs))\n",
    "\n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        # Compute activations and outputs of hidden units: H = sigmoid(W_0*X + b0)\n",
    "        H1_i = np.dot(X, self.W0) + self.b0\n",
    "        H1_o = self.sigmoid(H1_i)\n",
    "        # Compute activations and outputs of output units: Y = sigmoid(W_1*H + b1)\n",
    "        Y_i = np.dot(H1_o, self.W1) + self.b1\n",
    "        Y_o = self.sigmoid(Y_i)\n",
    "\n",
    "        outputs = {\n",
    "                \"H1_o\": H1_o,\n",
    "                \"Y_o\": Y_o,\n",
    "                }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def cost(self, y_true, y_predict, n_samples):\n",
    "        \"\"\"\n",
    "        Computes and returns the cost over all examples\n",
    "        \"\"\"\n",
    "        # same cost function as in logistic regression\n",
    "        cost = (- 1 / n_samples) * np.sum(y_true * np.log(y_predict) + (1 - y_true) * (np.log(1 - y_predict)))\n",
    "        cost = np.squeeze(cost)\n",
    "        assert isinstance(cost, float)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def backward_pass(self, X, Y, n_samples, outputs):\n",
    "        Y_o = outputs['Y_o']\n",
    "        H1_o = outputs['H1_o']\n",
    "        \n",
    "        # e2 = Y_o - Y\n",
    "        # e1 = W1*e2*H1*(1-H1)        -----    VERY IMPORTANT: *h2*(1-h2) is SCALING\n",
    "        # dJ/dw1 = H1*e2\n",
    "        # dJ/dw0 = X*e1\n",
    "        \n",
    "        e2 = Y_o - Y\n",
    "        dW1 = (1 / n_samples) * np.dot(H1_o.T, e2)\n",
    "        db1 = (1 / n_samples) * np.sum(e2)\n",
    "# -----VERY IMPORTANT: *H1*(1-H1) is SCALING NOT PRODUCTION: *H1*(1-H1) √ | .dot(H1.dot(1-H1)) X\n",
    "#         print(Y_o.shape)\n",
    "#         print(Y)\n",
    "#         print(e2.shape)\n",
    "#         print(self.W1.shape)\n",
    "#         print(H1_o.shape)\n",
    "        e1 = (np.dot(e2, self.W1.T)) * H1_o * (1 - H1_o)\n",
    "\n",
    "        dW0 = (1 / n_samples) * np.dot(X.T, e1)\n",
    "        db0 = (1 / n_samples) * np.sum(e1)\n",
    "\n",
    "        gradients = {\n",
    "                \"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW0\": dW0,\n",
    "                \"db0\": db0,\n",
    "                }\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_weights(self, gradients, eta):\n",
    "        self.W1 = self.W1 - eta * gradients[\"dW1\"]\n",
    "        self.b1 = self.b1 - eta * gradients[\"db1\"]\n",
    "        \n",
    "        self.W0 = self.W0 - eta * gradients[\"dW0\"]\n",
    "        self.b0 = self.b0 - eta * gradients[\"db0\"]\n",
    "\n",
    "    def train(self, X, y, n_iters=500, eta=0.3):\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            outputs = self.forward_pass(X)\n",
    "            cost = self.cost(y, outputs['Y_o'], n_samples=n_samples)\n",
    "            gradients = self.backward_pass(X, y, n_samples, outputs)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "#             if i % 1 == 0:\n",
    "                print(f'Cost at iteration {i}: {np.round(cost, 4)}')\n",
    "            self.update_weights(gradients, eta)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward_pass(X)\n",
    "        y_pred = [1 if elem >= 0.5 else 0 for elem in outputs[\"Y_o\"]]\n",
    "        return np.array(y_pred)[:, np.newaxis]\n",
    "    def predict_proba(self, X):\n",
    "        outputs = self.forward_pass(X)\n",
    "        return outputs['Y_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetH2():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden1, n_hidden2):\n",
    "        # Initialize weight matrices and bias vectors\n",
    "        self.W0 = np.random.randn(n_inputs, n_hidden1)\n",
    "        self.b0 = np.zeros((1, n_hidden1))\n",
    "        self.W1 = np.random.randn(n_hidden1, n_hidden2)\n",
    "        self.b1 = np.zeros((1, n_hidden2))\n",
    "        self.W2 = np.random.randn(n_hidden2, n_outputs)\n",
    "        self.b2 = np.zeros((1, n_outputs))\n",
    "\n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        # Compute activations and outputs of hidden units: H = sigmoid(W_0*X + b0)\n",
    "        H1_i = np.dot(X, self.W0) + self.b0\n",
    "        H1_o = self.sigmoid(H1_i)\n",
    "        # Compute activations and outputs of output units: Y = sigmoid(W_1*H + b1)\n",
    "        H2_i = np.dot(H1_o, self.W1) + self.b1\n",
    "        H2_o = self.sigmoid(H2_i)\n",
    "        \n",
    "        Y_i = np.dot(H2_o, self.W2) + self.b2\n",
    "        Y_o = self.sigmoid(Y_i)\n",
    "\n",
    "        outputs = {\n",
    "                \"H1_o\": H1_o,\n",
    "                \"H2_o\": H2_o,\n",
    "                \"Y_o\": Y_o,\n",
    "                }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def cost(self, y_true, y_predict, n_samples):\n",
    "        \"\"\"\n",
    "        Computes and returns the cost over all examples\n",
    "        \"\"\"\n",
    "        # same cost function as in logistic regression\n",
    "        cost = (- 1 / n_samples) * np.sum(y_true * np.log(y_predict) + (1 - y_true) * (np.log(1 - y_predict)))\n",
    "        cost = np.squeeze(cost)\n",
    "        assert isinstance(cost, float)\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def backward_pass(self, X, Y, n_samples, outputs):\n",
    "        Y_o = outputs['Y_o']\n",
    "        H2_o = outputs['H2_o']\n",
    "        H1_o = outputs['H1_o']\n",
    "        \n",
    "        # e2 = Y_o - Y\n",
    "        # e1 = W1*e2*H1*(1-H1)        -----    VERY IMPORTANT: *h2*(1-h2) is SCALING\n",
    "        # dJ/dw1 = H1*e2\n",
    "        # dJ/dw0 = X*e1\n",
    "        \n",
    "        e3 = Y_o - Y\n",
    "        dW2 = (1 / n_samples) * np.dot(H2_o.T, e3)\n",
    "        db2 = (1 / n_samples) * np.sum(e3)\n",
    "# -----VERY IMPORTANT: *H1*(1-H1) is SCALING NOT PRODUCTION: *H1*(1-H1) √ | .dot(H1.dot(1-H1)) X\n",
    "        e2 = (np.dot(e3, self.W2.T)) * H2_o * (1 - H2_o)\n",
    "        dW1 = (1 / n_samples) * np.dot(H1_o.T, e2)\n",
    "        db1 = (1 / n_samples) * np.sum(e2)\n",
    "\n",
    "        e1 = (np.dot(e2, self.W1.T)) * H1_o * (1 - H1_o)\n",
    "        dW0 = (1 / n_samples) * np.dot(X.T, e1)\n",
    "        db0 = (1 / n_samples) * np.sum(e1)\n",
    "        \n",
    "        gradients = {\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2,\n",
    "                \"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW0\": dW0,\n",
    "                \"db0\": db0,\n",
    "                }\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_weights(self, gradients, eta):\n",
    "        self.W2 = self.W2 - eta * gradients[\"dW2\"]\n",
    "        self.b2 = self.b2 - eta * gradients[\"db2\"]\n",
    "        \n",
    "        self.W1 = self.W1 - eta * gradients[\"dW1\"]\n",
    "        self.b1 = self.b1 - eta * gradients[\"db1\"]\n",
    "        \n",
    "        self.W0 = self.W0 - eta * gradients[\"dW0\"]\n",
    "        self.b0 = self.b0 - eta * gradients[\"db0\"]\n",
    "\n",
    "    def train(self, X, y, n_iters=500, eta=0.3):\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            outputs = self.forward_pass(X)\n",
    "            cost = self.cost(y, outputs['Y_o'], n_samples=n_samples)\n",
    "            gradients = self.backward_pass(X, y, n_samples, outputs)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'Cost at iteration {i}: {np.round(cost, 4)}')\n",
    "            self.update_weights(gradients, eta)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward_pass(X)\n",
    "        y_pred = [1 if elem >= 0.5 else 0 for elem in outputs[\"Y_o\"]]\n",
    "        return np.array(y_pred)[:, np.newaxis]\n",
    "    def predict_proba(self, X):\n",
    "        outputs = self.forward_pass(X)\n",
    "        return outputs['Y_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0: 0.694\n",
      "Cost at iteration 100: 0.6876\n",
      "Cost at iteration 200: 0.6692\n",
      "Cost at iteration 300: 0.6315\n",
      "Cost at iteration 400: 0.5807\n",
      "Cost at iteration 500: 0.5366\n",
      "Cost at iteration 600: 0.49\n",
      "Cost at iteration 700: 0.4169\n",
      "Cost at iteration 800: 0.3277\n",
      "Cost at iteration 900: 0.2629\n",
      "Cost at iteration 1000: 0.2239\n",
      "Cost at iteration 1100: 0.1981\n",
      "Cost at iteration 1200: 0.1798\n",
      "Cost at iteration 1300: 0.1662\n",
      "Cost at iteration 1400: 0.1558\n",
      "Cost at iteration 1500: 0.1476\n",
      "Cost at iteration 1600: 0.1409\n",
      "Cost at iteration 1700: 0.1353\n",
      "Cost at iteration 1800: 0.1305\n",
      "Cost at iteration 1900: 0.1264\n",
      "[[119   2]\n",
      " [  5 124]]\n",
      "precision:  0.9841269841269841\n",
      "recall:  0.9612403100775194\n",
      "f1-score:  0.9725490196078431\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:  0.9953872765712088\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on test set: 97.2 %\n"
     ]
    }
   ],
   "source": [
    "n_iters = 1000\n",
    "learning_rate = 0.1\n",
    "nn = NeuralNetH1(n_inputs=2, n_outputs=1, n_hidden1=6)\n",
    "# nn = NeuralNetH2(n_inputs=2, n_outputs=1, n_hidden1=6, n_hidden2=6)\n",
    "nn.train(X_train, y_train, n_iters=2000, eta=0.7)\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# plt.plot(np.arange(n_iters), costs)\n",
    "# plt.title(\"Development of cost during training\")\n",
    "# plt.xlabel(\"Number of iterations\")\n",
    "# plt.ylabel(\"Cost\")\n",
    "# plt.show()\n",
    "\n",
    "n_samples, _ = X_train.shape\n",
    "n_samples_test, _ = X_test.shape\n",
    "\n",
    "# y_p_train = nn.predict(X_train)\n",
    "y_p_test = nn.predict(X_test)\n",
    "\n",
    "# evaluate_binary_clf_bin(y_train, y_p_train)\n",
    "evaluate_binary_clf_bin(y_test, y_p_test)\n",
    "\n",
    "# y_proba_train = nn.predict_proba(X_train)\n",
    "y_proba_test = nn.predict_proba(X_test)\n",
    "\n",
    "# evaluate_binary_clf_proba(y_train, y_proba_train)\n",
    "evaluate_binary_clf_proba(y_test, y_proba_test)\n",
    "n_test_samples, _ = X_test.shape\n",
    "y_predict = nn.predict(X_test)\n",
    "print(f\"Classification accuracy on test set: {(np.sum(y_predict == y_test)/n_test_samples)*100} %\")\n",
    "\n",
    "# X_temp, y_temp = make_circles(n_samples=60000, noise=.5)\n",
    "# y_predict_temp = nn.predict(X_temp)\n",
    "# y_predict_temp = np.ravel(y_predict_temp)\n",
    "\n",
    "# fig = plt.figure(figsize=(8,12))\n",
    "# ax = fig.add_subplot(2,1,1)\n",
    "# plt.scatter(X[:,0], X[:,1], c=y)\n",
    "# plt.xlim([-1.5, 1.5])\n",
    "# plt.ylim([-1.5, 1.5])\n",
    "# plt.xlabel(\"First feature\")\n",
    "# plt.ylabel(\"Second feature\")\n",
    "# plt.title(\"Training and test set\")\n",
    "\n",
    "# ax = fig.add_subplot(2,1,2)\n",
    "# plt.scatter(X_temp[:,0], X_temp[:,1], c=y_predict_temp)\n",
    "# plt.xlim([-1.5, 1.5])\n",
    "# plt.ylim([-1.5, 1.5])\n",
    "# plt.xlabel(\"First feature\")\n",
    "# plt.ylabel(\"Second feature\")\n",
    "# plt.title(\"Decision boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Combining W & b: somewhere buggy\n",
    "# class NeuralNetwork:\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def sigmoid(self, z):\n",
    "#         return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "#     def append_bias(self, X):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         ones = np.ones(shape=(n_samples, 1))\n",
    "#         X1 = np.concatenate((X, ones),axis = 1)\n",
    "#         return X1\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         Computes and returns network predictions for given dataset\n",
    "#         \"\"\"\n",
    "#         X1 = self.append_bias(X)\n",
    "#         h_o, y_o = self.forward_pass(X1)\n",
    "#         y_bin = [1 if elem >= 0.5 else 0 for elem in y_o]\n",
    "\n",
    "#         return np.array(y_bin)[:, np.newaxis]\n",
    "    \n",
    "#     def predict_proba(self, X):\n",
    "#         X1 = self.append_bias(X)\n",
    "#         h_o, y_o = self.forward_pass(X1)\n",
    "#         print(h_o)\n",
    "#         return y_o\n",
    "    \n",
    "#     def forward_pass(self, X1):\n",
    "        \n",
    "#         \"\"\"\n",
    "#         Propagates the given input X forward through the net.\n",
    "#         \"\"\"\n",
    "#         # Compute activations and outputs of hidden units\n",
    "        \n",
    "#         h_i = np.dot(X1, self.w1)\n",
    "#         h_o = self.append_bias(self.sigmoid(h_i))\n",
    "# #         h_o = self.sigmoid(h_i)\n",
    "\n",
    "#         y_i = np.dot(h_o, self.w2)\n",
    "#         y_o = self.sigmoid(y_i)\n",
    "#         self.debug = False\n",
    "#         if self.debug:\n",
    "#             print(X1.shape)\n",
    "#             print(self.w1.shape)\n",
    "#             print(h_i.shape)\n",
    "#             print(h_o.shape)\n",
    "#             print(self.w2.shape)\n",
    "#             print(y_i.shape)\n",
    "#             print(y_o.shape)\n",
    "        \n",
    "#         return h_o, y_o\n",
    "    \n",
    "#     def train_gradient_descent(self, X, y, learning_rate=0.01, n_iters=100):\n",
    "#         # Step 0: Initialize the parameters\n",
    "#         # 3 layers: w1, w2\n",
    "#         n_samples, n_features = X.shape\n",
    "#         X1 = self.append_bias(X)   \n",
    "\n",
    "#         nodes_num_layer1 = 6\n",
    "        \n",
    "#         self.debug = False\n",
    "#         self.w1 = np.random.rand(n_features + 1, nodes_num_layer1)\n",
    "# #         print('W1', w1.shape)\n",
    "        \n",
    "#         # layer 2 parameter number can be configed \n",
    "#         self.w2 = np.random.rand(nodes_num_layer1 + 1, 1)\n",
    "    \n",
    "#         costs = []\n",
    "#         # Step 1: Loop until iterate enoughf\n",
    "    \n",
    "#         for k in range(n_iters):\n",
    "#             # forward predict impls\n",
    "#             h_o, y_o = self.forward_pass(X1)\n",
    "            \n",
    "#             # e3 = y - yr\n",
    "#             # e2 = w2*e3*h2*(1-h2)        -----    VERY IMPORTANT: *h2*(1-h2) is SCALING\n",
    "#             # dJ/dw2 = h2*e3\n",
    "#             # dJ/dw1 = h1*e2\n",
    "            \n",
    "#             e3 = y_o - y\n",
    "# #             e2 = np.dot(np.dot(e3, self.w2.T), np.dot(h_o.T, 1 - h_o))\n",
    "# #             m, n = h_o.shape\n",
    "# #             h_o_S = h_o[:,:n]\n",
    "#             e2 = np.dot(e3, self.w2.T) * h_o * (1 - h_o)\n",
    "        \n",
    "# #             print(h_o)\n",
    "# #             print(1-h_o)\n",
    "# #             print(np.dot(h_o.T, 1-h_o))\n",
    "#             # Step 2: Compute cost over training set\n",
    "#             cost = -(1 / n_samples) * np.sum(y * np.log(y_o) + (1 - y) * np.log(1 - y_o))\n",
    "#             costs.append(cost)\n",
    "#             if k % (n_iters / 10) == 0:\n",
    "#                 print(f\"Cost at iteration {k}: {cost}\")\n",
    "#             # Step 3: Compute the gradients\n",
    "#             dJ_dw2 = 1.0 / n_samples * np.dot(h_o.T, e3)\n",
    "\n",
    "#             m, n = w1.shape\n",
    "#             dJ_dw1 = 1.0 / n_samples * np.dot(X1.T, e2[:,:n])\n",
    "\n",
    "# #             gradient = get_gradient()\n",
    "#             # Step 4: Update the parameters\n",
    "\n",
    "#             if self.debug:\n",
    "#                 print(e3.shape)\n",
    "#                 print(self.w2.shape)\n",
    "#                 print(h_o.shape)\n",
    "#                 print((1-h_o).shape)\n",
    "#                 print(e2.shape)\n",
    "#                 print(dJ_dw2.shape)\n",
    "#                 print(dJ_dw1.shape)\n",
    "                \n",
    "#             self.w2 = self.w2 - learning_rate * dJ_dw2\n",
    "#             self.w1 = self.w1 - learning_rate * dJ_dw1\n",
    "#         return self.w1, self.w2, costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
