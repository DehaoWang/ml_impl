{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "N_STATES = 10   # 1维世界的宽度\n",
    "ACTIONS = ['left', 'right']     # 探索者的可用动作\n",
    "EPSILON = 0.9   # 贪婪度 greedy\n",
    "ALPHA = 0.1     # 学习率\n",
    "GAMMA = 0.9    # 奖励递减值\n",
    "MAX_EPISODES = 13   # 最大回合数\n",
    "FRESH_TIME = 0.3    # 移动间隔时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table 全 0 初始\n",
    "        columns=actions,    # columns 对应的是行为名称\n",
    "    )\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在某个 state 地点, 选择行为\n",
    "def choose_action(state, q_table):\n",
    "    state_actions = q_table.iloc[state, :]  # 选出这个 state 的所有 action 值\n",
    "    if (np.random.uniform() > EPSILON) or (state_actions.all() == 0):  # 非贪婪 or 或者这个 state 还没有探索过\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        action_name = state_actions.argmax()    # 贪婪模式\n",
    "    return action_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl():\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)  # 初始 q table\n",
    "    for episode in range(MAX_EPISODES):     # 回合\n",
    "        step_counter = 0\n",
    "        S = 0   # 回合初始位置\n",
    "        is_terminated = False   # 是否回合结束\n",
    "        update_env(S, episode, step_counter)    # 环境更新\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)   # 选行为\n",
    "            S_, R = get_env_feedback(S, A)  # 实施行为并得到环境的反馈\n",
    "            q_predict = q_table.loc[S, A]    # 估算的(状态-行为)值\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   #  实际的(状态-行为)值 (回合没结束)\n",
    "            else:\n",
    "                q_target = R     #  实际的(状态-行为)值 (回合结束)\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  #  q_table 更新\n",
    "            S = S_  # 探索者移动到下一个 state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)  # 环境更新\n",
    "\n",
    "            step_counter += 1\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----o---T                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtdp1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Q-table:\n",
      "\n",
      "           left         right\n",
      "0  1.461524e-09  8.014537e-07\n",
      "1  4.972157e-08  6.992230e-06\n",
      "2  2.493869e-08  7.963063e-05\n",
      "3  4.998203e-08  9.975457e-04\n",
      "4  6.685312e-05  5.328969e-03\n",
      "5  5.904900e-07  2.907183e-02\n",
      "6  3.018060e-05  1.181041e-01\n",
      "7  6.240240e-04  3.509570e-01\n",
      "8  6.598260e-03  7.458134e-01\n",
      "9  0.000000e+00  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
